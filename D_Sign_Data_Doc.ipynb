{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61153c5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Tomato;\">Data Science course of DHDK university program - Academic year 2021-2022</h2>\n",
    "\n",
    "<h1>D_Sign_Data Notebook</h1>\n",
    "<h3><i>Descriptive document of the final project work</i></h3>\n",
    "\n",
    "<em>Camila Oliveira, Anita Vishinskaite, Marida Di Lembo</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae37698",
   "metadata": {},
   "source": [
    "<h2>The Context</h2>\n",
    "\n",
    "<blockquote>The goal of the project is to develop a software that enables one to process data stored in different formats and to upload them into two distinct databases to query these databases simultaneously according to predefined operations. </blockquote>\n",
    "<br>\n",
    "<p>We have been analysing the project requirements and the data provided and assigned a different task to each of us for the development activity</p>\n",
    "<p>Basically we splitted in two groups: one focused on the relational database and the other on the graph database.</p>\n",
    "<p>The tasks were developed in the following way:<p>\n",
    "    <ul>\n",
    "        <li>Camila Oliveiraaaa: Relational Processor, Relational Data Processor and Relational Query Processor</li>\n",
    "        <li>Anita Vishinskaite: Triple Store Processor, Triple Store Data Processor</li>\n",
    "        <li>Marida Di Lembo: Triple Store Query Processor, Generic Query Data Processor</li>\n",
    "</ul>\n",
    "<p><em>Initially the group was composed of four members, at the end we remained in three.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f334d7b",
   "metadata": {},
   "source": [
    "<h3>Tools and collaboration environment</h3>\n",
    "\n",
    "<p>First of all we have been setting up our Github Repository: <a href=\"https://github.com/lelax/D_Sign_Data\">https://github.com/lelax/D_Sign_Data</a> and shared access to it in order to be able to upload the material and syncronize it in real time.</p>\n",
    "<p>Then we have been also evaluating between Jupyter and Google Colab and we finally decided to use Jupyter for the documentation report.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5daa1b",
   "metadata": {},
   "source": [
    "<h4><em>What you will find in the repository?</em></h4>\n",
    "\n",
    "<ul>\n",
    "    <li>A folder called <b>import</b> where we have copied the .csv and .json files provided</li>\n",
    "    <li>A folder called <b>URIref</b> where we have created additional classes</li>\n",
    "    <li>The <b>impl.py</b> Python file, where all the relational data base has been defined</li>\n",
    "    <li>The <b>graph_01.py</b> Python file, where all the graph data base has been defined</li>\n",
    "    <li>A file for the SPARQL queries</li>\n",
    "    <li>A file for\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ec343",
   "metadata": {},
   "source": [
    "<h2>The Relational Workflow</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508dfd3",
   "metadata": {},
   "source": [
    "In the <em>impl.py</em> file we have been developing the code for the creation of a relational workflow of the data model requested and the data provided.\n",
    "<p>The libraries imported and used are the following:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a2f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlite3 import connect\n",
    "from pandas import read_csv\n",
    "from pandas import Series\n",
    "from pandas import read_sql\n",
    "from pandas import read_json\n",
    "from pandas import DataFrame\n",
    "from csv import reader\n",
    "from pandas import merge\n",
    "from json import dump\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a29b07",
   "metadata": {},
   "source": [
    "<h2>UML of data model classes<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd3e28",
   "metadata": {},
   "source": [
    "<p>The following classes were created as well as their methods as specified in the model:</p>\n",
    "<ul>\n",
    "    <li><b>Identifiable Entity:</b> being the super class</li>\n",
    "    <li>Person</li>\n",
    "    <li>Publication</li>\n",
    "    <li>Venue</li>\n",
    "    <li>Oragnization</li>\n",
    "    <li>Journal Article</li>\n",
    "    <li>Book Chapter</li>\n",
    "    <li>Proceedings Paper</li>\n",
    "    <li>Journal</li>\n",
    "    <li>Book</li>\n",
    "    <li>Proceedings</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8c744",
   "metadata": {},
   "source": [
    "<h2>Class Relational Processor</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da687284",
   "metadata": {},
   "source": [
    "<p>From the Relational Processor class the attributes getDbPath and setDbPath can be used to return the path of the database and enable a new database path. Those are used in the following way:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"relational.db\"\n",
    "rel_dp = RelationalDataProcessor(rel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f210af",
   "metadata": {},
   "source": [
    "<em>It's important to use them in the order described. </em> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215351e2",
   "metadata": {},
   "source": [
    "<h2>Class Relational Data Processor</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2568b2c",
   "metadata": {},
   "source": [
    "<p>In this class you'll be able to find the uploadData method in which allows one to upload data, either in CSV or JSON file, into the database previously created with the methods from the Relational Processor class. And can be used as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39909fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dp.uploadData(\"relational_publications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f08a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dp.uploadData(\"relational_other_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36228df2",
   "metadata": {},
   "source": [
    "<p>After the database path is created and data is uploaded the following tables are created in the relational database:</p>\n",
    "<ul>\n",
    "    <li>Authors</li>\n",
    "    <li>Venues Id</li>\n",
    "    <li>References</li>\n",
    "    <li>Publishers</li>\n",
    "    <li>Publications</li>\n",
    "    <li>Journal Articles</li>\n",
    "    <li>Organization</li>\n",
    "    <li>Journals</li>\n",
    "    <li>Books</li>\n",
    "    <li>Proceedings</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceeba27",
   "metadata": {},
   "source": [
    "<h2>Class Relational Query Processor</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ffc71",
   "metadata": {},
   "source": [
    "<p>In this class, one is able to query information from the database created according to the follwing methods:</p>\n",
    "<ul>\n",
    "    <li>getPublicationsPublishedinYear</li>\n",
    "    <li>getPublicationsByAuthorId</li>\n",
    "    <li>getMostCitedPublication</li>\n",
    "    <li>getMostCitedVenue</li>\n",
    "    <li>getVenuesByPublisherId</li>\n",
    "    <li>getPublicationInVenue</li>\n",
    "    <li>getJournalArticleInIssue</li>\n",
    "    <li>getJournalArticlesInVolume</li>\n",
    "    <li>getJournalArticlesInJournal</li>\n",
    "    <li>getProceedingsByEvent</li>\n",
    "    <li>getPublicationAuthors</li>\n",
    "    <li>getPublicationByAuthorName</li>\n",
    "    <li>getDistinctPublisherOfPublications</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95af51",
   "metadata": {},
   "source": [
    "<h2>The Graph workflow</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc0fd5",
   "metadata": {},
   "source": [
    "Class TriplestoreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriplestoreProcessor(object):\n",
    "    def __init__(self, endpointUrl=\"\"): # the variable containing the URL of the SPARQL endpoint of the triplestore, initially set as an empty string, that will be updated with the method setEndpointUrl\n",
    "        self.endpointUrl = endpointUrl\n",
    "        \n",
    "    # Methods:\n",
    "    def getEndpointUrl(self):  # it returns the path of the database\n",
    "        return self.endpointUrl\n",
    "\n",
    "    def setEndpointUrl(self, newURL): # it enables to set a new URL for the SPARQL endpoint of the triplestore.\n",
    "        self.endpointUrl = newURL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea8fcf",
   "metadata": {},
   "source": [
    "Class TriplestoreDataProcessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3df80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriplestoreDataProcessor(object):\n",
    "    def __init__(self):\n",
    "        self.Data = None\n",
    "\n",
    "    # Method:\n",
    "    def uploadData(self, Data): # it enables to upload the collection of data specified in the input file path (either in CSV or JSON) into the database.\n",
    "        self.Data = Data\n",
    "        with open(self.Data, \"r\", encoding=\"utf-8\") as f:\n",
    "            Data = reader(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c54ec",
   "metadata": {},
   "source": [
    "The first step of the creation of the data frame based on the RDF triplestore language has been the definition of all the classes, with attributes and relations. For doing this we needed to import the <em>Graph</em> and the <em>URIRef</em> components from <b>RDF library</b>. They allow to setup an empty graph ready to be populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3986db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "my_graph = Graph()\n",
    "\n",
    "from rdflib import URIRef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7f214",
   "metadata": {},
   "source": [
    "Then basically looking at the UML model provided we have been listing all the classes required for the resources and assigning for each class a URI, using the resources of schema.org and Fabio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c87358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes of resources\n",
    "JournalArticle = URIRef(\"https://schema.org/ScholarlyArticle\")\n",
    "BookChapter = URIRef(\"https://schema.org/Chapter\")\n",
    "ProceedingsPaper = URIRef(\"http://purl.org/spar/fabio/ProceedingsPaper\")\n",
    "Journal = URIRef(\"https://schema.org/Periodical\")\n",
    "Book = URIRef(\"https://schema.org/Book\")\n",
    "Proceedings = URIRef(\"http://purl.org/spar/fabio/AcademicProceedings\")\n",
    "Publication = URIRef(\"https://schema.org/publication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c13b",
   "metadata": {},
   "source": [
    "Same process has been done for attributes and relations among classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes related to classes\n",
    "publicationYear = URIRef(\"https://schema.org/datePublished\")\n",
    "title = URIRef(\"http://purl.org/dc/terms/title\")\n",
    "issue = URIRef(\"https://schema.org/issueNumber\")\n",
    "volume = URIRef(\"https://schema.org/volumeNumber\")\n",
    "identifier = URIRef(\"https://schema.org/identifier\")\n",
    "name = URIRef(\"https://schema.org/name\")\n",
    "event = URIRef(\"https://schema.org/Event\")\n",
    "chapterNumber = URIRef(\"https://github.com/lelax/D_Sign_Data/blob/main/URIRef/chapterNumber\")\n",
    "givenName = URIRef (\"https://schema.org/givenName\")\n",
    "familyName = URIRef (\"https://schema.org/familyName\")\n",
    "\n",
    "# relations among classes\n",
    "publicationVenue = URIRef(\"https://schema.org/isPartOf\")\n",
    "publisher = URIRef (\"https://schema.org/publishedBy\")\n",
    "author = URIRef (\"http://purl.org/saws/ontology#isWrittenBy\")\n",
    "cites = URIRef (\"https://schema.org/citation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1b014",
   "metadata": {},
   "source": [
    "Values to specify as objects of RDF statements can be created using the class Literal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7177c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Literal\n",
    "\n",
    "a_string = Literal(\"a string\")\n",
    "a_number = Literal(42)\n",
    "a_boolean = Literal(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ff2d9",
   "metadata": {},
   "source": [
    "The following code show how we populates the RDF graph defining using the data obtained by processing the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5421b1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'graph_publications.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\anita\\AppData\\Local\\Temp\\Temp1_D_Sign_Data-main (2).zip\\D_Sign_Data-main\\D_Sign_Data_Doc.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrdflib\u001b[39;00m \u001b[39mimport\u001b[39;00m RDF\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=3'>4</a>\u001b[0m base_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/lelax/D_Sign_Data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=5'>6</a>\u001b[0m publications \u001b[39m=\u001b[39m read_csv(\u001b[39m\"\u001b[39;49m\u001b[39mgraph_publications.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=6'>7</a>\u001b[0m                  keep_default_na\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=7'>8</a>\u001b[0m                  dtype\u001b[39m=\u001b[39;49m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=8'>9</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=9'>10</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=10'>11</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=11'>12</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mpublication_year\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=12'>13</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mpublication_venue\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=13'>14</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39missue\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=14'>15</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mvolume\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=15'>16</a>\u001b[0m                      \u001b[39m\"\u001b[39;49m\u001b[39mchapter\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=16'>17</a>\u001b[0m                   })\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, row \u001b[39min\u001b[39;00m publications\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000039?line=19'>20</a>\u001b[0m     local_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpublication-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(idx)\n",
      "File \u001b[1;32mc:\\Users\\anita\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anita\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\anita\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\anita\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\anita\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\anita\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'graph_publications.csv'"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv, Series\n",
    "from rdflib import RDF\n",
    "\n",
    "base_url = \"https://github.com/lelax/D_Sign_Data\"\n",
    "\n",
    "publications = read_csv(\"graph_publications.csv\", \n",
    "                 keep_default_na=False,\n",
    "                 dtype={\n",
    "                     \"id\": \"string\",\n",
    "                     \"title\": \"string\",\n",
    "                     \"type\": \"string\",\n",
    "                     \"publication_year\": \"int\",\n",
    "                     \"publication_venue\": \"string\",\n",
    "                     \"issue\": \"string\",\n",
    "                     \"volume\": \"string\",\n",
    "                     \"chapter\": \"string\",\n",
    "                  })\n",
    "\n",
    "for idx, row in publications.iterrows():\n",
    "    local_id = \"publication-\" + str(idx)\n",
    "    \n",
    "    subj = URIRef(base_url + local_id)\n",
    "\n",
    "    if row[\"type\"] == \"journal-article\":\n",
    "        my_graph.add((subj, RDF.type, JournalArticle))\n",
    "        # These two statements applies only to journal articles\n",
    "        my_graph.add((subj, issue, Literal(row[\"issue\"])))\n",
    "        my_graph.add((subj, volume, Literal(row[\"volume\"])))\n",
    "    elif row[\"type\"] == \"book-chapter\":\n",
    "        my_graph.add((subj, RDF.type, BookChapter))\n",
    "        #This statement applies only to book chapters\n",
    "        my_graph.add((subj, chapterNumber, Literal(row[\"chapter\"])))\n",
    "    else: \n",
    "        my_graph.add((subj, RDF.type, ProceedingsPaper))\n",
    "\n",
    "    my_graph.add((subj, cites, Publication))\n",
    "    my_graph.add((subj, title, Literal(row[\"title\"])))\n",
    "    my_graph.add((subj, publicationYear, Literal(str(row[\"publication_year\"]))))\n",
    "    my_graph.add((subj, publicationVenue, Literal(row[\"publication_venue\"])))\n",
    "    my_graph.add((subj, identifier, Literal(str(row[\"id\"]))))\n",
    "\n",
    "\n",
    "\n",
    "venues = read_csv(\"graph_publications.csv\", \n",
    "                 keep_default_na=False,\n",
    "                 dtype={\n",
    "                     \"venue_type\": \"string\",\n",
    "                     \"publisher\": \"string\",\n",
    "                     \"event\": \"string\"\n",
    "                  })\n",
    "\n",
    "for idx, row in venues.iterrows():\n",
    "    local_id = \"venues-\" + str(idx)\n",
    "    \n",
    "    subj = URIRef(base_url + local_id)\n",
    "\n",
    "    if row[\"venue_type\"] == \"journal\":\n",
    "        my_graph.add((subj, RDF.type, Journal))\n",
    "    elif row[\"venue_type\"] == \"book\":\n",
    "        my_graph.add((subj, RDF.type, BookChapter))\n",
    "    else:\n",
    "        my_graph.add((subj, RDF.type, Proceedings))\n",
    "        #This statement applies only to proceedings\n",
    "        my_graph.add((subj, event, Literal(row[\"event\"])))\n",
    "\n",
    "    my_graph.add((subj, publisher, Literal(row[\"publisher\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c393255",
   "metadata": {},
   "source": [
    "We can now test how many RDF triples we added to the RDF graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa599343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Number of triples added to the graph after processing the venues\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\anita\\AppData\\Local\\Temp\\Temp1_D_Sign_Data-main (2).zip\\D_Sign_Data-main\\D_Sign_Data_Doc.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000041?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-- Number of triples added to the graph after processing the venues\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/anita/AppData/Local/Temp/Temp1_D_Sign_Data-main%20%282%29.zip/D_Sign_Data-main/D_Sign_Data_Doc.ipynb#ch0000041?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(my_graph))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_graph' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"-- Number of triples added to the graph after processing the venues\")\n",
    "print(len(my_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0406ce",
   "metadata": {},
   "source": [
    "Then we have imported also the constructor <em>Literal</em> for the string, number and boolean variables to be used as attributes of the triples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b95ad",
   "metadata": {},
   "source": [
    "At this point we could start the implementation of the <b>RDF Triplestore</b> and start testing and querying the data provided in the <em>graph_publication.csv</em> file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5cfd1",
   "metadata": {},
   "source": [
    "<h2>Class Generic Query Processor</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7589421",
   "metadata": {},
   "source": [
    "We have defined the class <em>GenericQueryProcessor</em> in order to define also the functions to collect all the query result and store them in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d61c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericQueryProcessor():\n",
    "    qps = []\n",
    "\n",
    "    def addQueryProcessor(qp):\n",
    "        qps.append(qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184c373",
   "metadata": {},
   "source": [
    "Then we added for each method a function calling it and appending all the result by using an iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19446e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2e683e5e2ed245449f7aa357b5a83d3cb3fb19968652fa58849717518e2b16c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
